# example for readme

Sprint의 각 노트마다 자신의 학습 경험을 정리하고 아는 것과 모르는 것을 회고하는 시간을 가져봅시다.  
그 과정에서 자신이 무엇을 모르는지 안다면 더욱 효율적으로 공부할 수 있을 것입니다.

아래 질문에 답하고 README를 정리해보세요.

```
1. 오늘 노트에서 새롭게 배운 개념은 무엇인가요?
2. 새롭게 배운 개념을 주요 키워드 위주로 요약합니다.
3. 학습 내용 중 이해가 가지 않는 키워드는 무엇인가요?
```

## N421

## N422

## N423

## N424

### 오늘의 키워드
- Transformer
- Encoder-Decoder
- Self-attention
- ...

### 개념 정리
- Transformer
  - 기계 번역(Machine Translation)을 위해 고안된 모델.
  - 순환 신경망(RNN)을 이용하지 않아 병렬 처리에 유리하고 모델의 구조가 단순함.
- Positional Encoding
  - 트랜스포머는 데이터를 순차적으로 입력받지 않기 때문에 위치 정보를 더해 입력해줄 필요가 있음.
  - 일정한 주기 값을 이용합니다.
- Self-Attention
  - 입력 문장 내 단어 간의 의미를 파악하기 위해 사용됩니다.
  - Scaled dot-product attention을 이용합니다.  
- ...

  
### 이해 가지 않는 키워드
- [ ] 일정한 패턴 값이 어떻게 위치 정보를 반영하게 될까?
- [ ] Attetnion sub-layer를 Multi-Head로 구성하는 이유가 뭘까?
- [ ] 두 종류의 Masking을 해주는 이유는 무엇일까?
- [ ] ...

### 추가 학습을 위한 Reference
- 자연어 처리의 발전과 역사
  - https://youtu.be/DUyjMQo9zkY 
